{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd04cf4a5c25d0443c718bc6df8c1cfde806d3459d5f26eb1cf93f3c76670b0736a",
   "display_name": "Python 3.9.2 64-bit ('drl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# --- Load Agents --- #\n",
    "from agents.agent_reinforce.agent import REINFORCEAgent\n",
    "from agents.agent_deepqn.agent import DeepQAgent\n",
    "from agents.agent_ddpg.agent import DDPG_Agent\n",
    "\n",
    "# --- Load Environments --- #\n",
    "from environment.grid import GridEnv\n",
    "\n",
    "# --- Load Necessary --- #\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from utilities.helper import flatten\n",
    "\n",
    "\n",
    "# --- Load Training --- #\n",
    "from experiments.experiment import Experiment\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "source": [
    "# Generalized"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Not all _agents_ have fixed starting positions, rest (1) will be random\nNot all _goals_ have fixed starting positions, rest (1) will be random\nNot all _agents_ have fixed starting positions, rest (2) will be random\nNot all _goals_ have fixed starting positions, rest (2) will be random\n"
     ]
    }
   ],
   "source": [
    "environments = [GridEnv(num_agent = 2, agents_start = [(1,1)], goals_start=[(7,7)]), GridEnv(num_agent = 2)]\n",
    "wrapped_agents = [[DeepQAgent(env.state_space, env.action_space.n) for n in range(env.num_agent)] for env in environments]\n",
    "# wrapped_agents = [[DeepQAgent(env.state_space, env.action_space.n, dddpqagent_config) for n in range(env.num_agent)] for env in environments]\n",
    "\n",
    "# dddpqagent_config=[{}, ]\n",
    "# print(environments)\n",
    "# print(wrapped_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_experiments=[Experiment(\"Test Experiment\", env, agents, max_t=100, num_episodes=300) for agents, env in zip(wrapped_agents, environments)]\n",
    "\n",
    "# dqn_experiments=[Experiment(\"Test Experiment\", env, agents, max_t=100, num_episodes=300) for agents, env in zip(wrapped_agents, environments)]\n",
    "\n",
    "# reinforce_experiments=[Experiment(\"Reinforce Experiment\", env, agents, max_t=100, num_episodes=300) for agents, env in zip(wrapped_agents, environments)]\n",
    "\n",
    "# print(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running experiment\n",
      " Total score (averaged over agents) 299 episode: -100.0 | \tAvarage in last 100 is -104.385"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2dd42c346185>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mddpg_experiments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mscore_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# experiment.save(score_history, state_history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "for experiment in ddpg_experiments:\n",
    "    score_history, state_history = experiment.run()\n",
    "    experiment.save(score_history, state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination=np.array(np.meshgrid(num_players, agents_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "#print(combination.T)\n",
    "import itertools\n",
    "print(len(list(itertools.product(*configs))))\n",
    "# for r in itertools.product(num_players, agents_start):\n",
    "#     for q in itertools.product(r, goals_start):\n",
    "#         for s in itertools.product(q, prob_right_direction):\n",
    "#             print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}