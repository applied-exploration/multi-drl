{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3612jvsc74a57bd021b14e6ae810b98687c42101764fbc6ed2f749f10b37141b42930ea65db4f89b",
   "display_name": "Python 3.6.12 64-bit ('drlnd': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:109.)\n  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# --- Load Agents --- #\n",
    "from agent_reinforce.agent import REINFORCEAgent\n",
    "from agent_deepqn.agent import DeepQAgent\n",
    "from agent_ddpg.a_agent import DDPG_Agent\n",
    "\n",
    "# --- Load Environments --- #\n",
    "from environment.grid import GridEnv\n",
    "\n",
    "\n",
    "# --- Load Necessary --- #\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from utils import flatten\n",
    "\n",
    "\n",
    "# --- Load Training --- #\n",
    "from training import train\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Unity Environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, state_space, action_space, brain_name, num_agents = environment_loader(\"environment/Reacher_Single/Reacher.exe\", no_graphics = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [DDPG_Agent(state_space, action_space)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(agents, env,  max_t=1, num_episodes = 1)"
   ]
  },
  {
   "source": [
    "# GRID Environment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridEnv(num_agent = 1, agents_start = [(1,1)], goals_start=[(7,7)])\n",
    "agents = [DeepQAgent(env.state_space, env.action_space.n)]\n",
    "train(agents, env)"
   ]
  },
  {
   "source": [
    "# Grid environment with DDPG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nFinal Self players:  [(1, 1)]\nFinal Self goals:  [(4, 4)]\n"
     ]
    }
   ],
   "source": [
    "env = GridEnv(num_agent = 1, agents_start = [(1,1)], goals_start=[(4,4)], grid_size = 5, render_board= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n--- Agent Params ---\nGoing to train on cpu\nLearning Rate:: Actor: 0.01 | Critic: 0.01\nReplay Buffer:: Buffer Size: 100000 | Sampled Batch size: 100\n\nActor paramaters:: Input: 4 | Hidden Layers: [16, 8] | Output: 4\nCritic paramaters:: Input: 4 | Hidden Layers: [20, 8] | Output: 1\nActor(\n  (fc_in): Linear(in_features=4, out_features=16, bias=True)\n  (hidden_layers): ModuleList(\n    (0): Linear(in_features=16, out_features=8, bias=True)\n  )\n  (fc_out): Linear(in_features=8, out_features=4, bias=True)\n)\nCritic(\n  (fc_in): Linear(in_features=4, out_features=16, bias=True)\n  (hidden_layers): ModuleList(\n    (0): Linear(in_features=20, out_features=8, bias=True)\n  )\n  (fc_out): Linear(in_features=8, out_features=1, bias=True)\n)\nOutput type is: probs\n\n\n"
     ]
    }
   ],
   "source": [
    "agents = [DDPG_Agent(env.state_space, env.action_space.n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "9.9999452e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2793623e-06 2.1972985e-06 1.9881213e-06 9.9999452e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2774409e-06 2.1940905e-06 1.9851298e-06 9.9999452e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2755102e-06 2.1908704e-06 1.9821236e-06 9.9999452e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2735679e-06 2.1876301e-06 1.9790996e-06 9.9999452e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 79 episode: -10.0 | \tAvarage in last 100 is -9.7125action_prob: [7.8121190e-07 1.3426479e-06 1.2093751e-06 9.9999666e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2696776e-06 2.1811413e-06 1.9730410e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2677344e-06 2.1779051e-06 1.9700178e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2657966e-06 2.1746714e-06 1.9669953e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2638558e-06 2.1714343e-06 1.9639738e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2619120e-06 2.1681938e-06 1.9609456e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2599700e-06 2.1649562e-06 1.9579222e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2580333e-06 2.1617273e-06 1.9549052e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2561127e-06 2.1585217e-06 1.9519114e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2541951e-06 2.1553253e-06 1.9489278e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 80 episode: -10.0 | \tAvarage in last 100 is -9.716049382716049action_prob: [7.6900574e-07 1.3222872e-06 1.1904370e-06 9.9999666e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2503758e-06 2.1489543e-06 1.9429781e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2484729e-06 2.1457822e-06 1.9400156e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2465764e-06 2.1426147e-06 1.9370614e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2446900e-06 2.1394706e-06 1.9341226e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2428040e-06 2.1363203e-06 1.9311847e-06 9.9999464e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2409307e-06 2.1331955e-06 1.9282700e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2390693e-06 2.1300852e-06 1.9253685e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2372154e-06 2.1269936e-06 1.9224824e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2353762e-06 2.1239227e-06 1.9196170e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 81 episode: -10.0 | \tAvarage in last 100 is -9.71951219512195action_prob: [7.5718958e-07 1.3025646e-06 1.1721053e-06 9.9999678e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2317116e-06 2.1178064e-06 1.9139100e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2298888e-06 2.1147628e-06 1.9110721e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2280709e-06 2.1117276e-06 1.9082420e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2262558e-06 2.1086969e-06 1.9054160e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2244551e-06 2.1056885e-06 1.9026087e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2226639e-06 2.1026965e-06 1.8998201e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2208893e-06 2.0997329e-06 1.8970572e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2191197e-06 2.0967775e-06 1.8943022e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2173583e-06 2.0938319e-06 1.8915582e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 82 episode: -10.0 | \tAvarage in last 100 is -9.72289156626506action_prob: [7.4588712e-07 1.2836807e-06 1.1545708e-06 9.9999678e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2138688e-06 2.0879972e-06 1.8861217e-06 9.9999475e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2121360e-06 2.0851003e-06 1.8834239e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2104033e-06 2.0822031e-06 1.8807262e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2086800e-06 2.0793257e-06 1.8780449e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2069719e-06 2.0764683e-06 1.8753852e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2052730e-06 2.0736263e-06 1.8727403e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2035775e-06 2.0707885e-06 1.8700988e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2018879e-06 2.0679624e-06 1.8674683e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.2001962e-06 2.0651323e-06 1.8648343e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 83 episode: -10.0 | \tAvarage in last 100 is -9.726190476190476action_prob: [7.3511086e-07 1.2656546e-06 1.1378575e-06 9.9999690e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1968290e-06 2.0595037e-06 1.8595935e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1951499e-06 2.0566929e-06 1.8569777e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1934779e-06 2.0538978e-06 1.8543744e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1918037e-06 2.0510950e-06 1.8517661e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1901375e-06 2.0483112e-06 1.8491737e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1884758e-06 2.0455295e-06 1.8465850e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1868086e-06 2.0427419e-06 1.8439874e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1851438e-06 2.0399598e-06 1.8413971e-06 9.9999487e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1834813e-06 2.0371760e-06 1.8388072e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 84 episode: -10.0 | \tAvarage in last 100 is -9.729411764705882action_prob: [7.2459795e-07 1.2480650e-06 1.1215475e-06 9.9999690e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1801768e-06 2.0316504e-06 1.8336622e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1785426e-06 2.0289165e-06 1.8311160e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1769196e-06 2.0261998e-06 1.8285874e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1752977e-06 2.0234866e-06 1.8260624e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1736803e-06 2.0207792e-06 1.8235443e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1720651e-06 2.0180792e-06 1.8210295e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1704577e-06 2.0153866e-06 1.8185252e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1688571e-06 2.0127052e-06 1.8160348e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1672597e-06 2.0100313e-06 1.8135477e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 85 episode: -10.0 | \tAvarage in last 100 is -9.732558139534884action_prob: [7.1442349e-07 1.2310309e-06 1.1057676e-06 9.9999690e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1640736e-06 2.0046939e-06 1.8085873e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1624938e-06 2.0020459e-06 1.8061259e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1609218e-06 1.9994127e-06 1.8036818e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1593573e-06 1.9967906e-06 1.8012443e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1577984e-06 1.9941758e-06 1.7988169e-06 9.9999499e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1562404e-06 1.9915647e-06 1.7963930e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1546845e-06 1.9889549e-06 1.7939706e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1531405e-06 1.9863637e-06 1.7915667e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1515964e-06 1.9837739e-06 1.7891643e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 86 episode: -10.0 | \tAvarage in last 100 is -9.735632183908047action_prob: [7.0459623e-07 1.2145490e-06 1.0905279e-06 9.9999702e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1485167e-06 1.9786044e-06 1.7843710e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1469843e-06 1.9760362e-06 1.7819867e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1454562e-06 1.9734675e-06 1.7796074e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1439289e-06 1.9709057e-06 1.7772330e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1424113e-06 1.9683587e-06 1.7748720e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1408947e-06 1.9658112e-06 1.7725139e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1393832e-06 1.9632705e-06 1.7701608e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1378847e-06 1.9607503e-06 1.7678276e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1363946e-06 1.9582480e-06 1.7655126e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 87 episode: -10.0 | \tAvarage in last 100 is -9.738636363636363action_prob: [6.9507172e-07 1.1985449e-06 1.0757672e-06 9.9999702e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1334313e-06 1.9532642e-06 1.7609018e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1319535e-06 1.9507772e-06 1.7586027e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1304842e-06 1.9483082e-06 1.7563199e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1290221e-06 1.9458460e-06 1.7540468e-06 9.9999511e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1275729e-06 1.9434074e-06 1.7517935e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1261264e-06 1.9409699e-06 1.7495413e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1246860e-06 1.9385445e-06 1.7473036e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1232497e-06 1.9361241e-06 1.7450704e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1218206e-06 1.9337142e-06 1.7428484e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 88 episode: -10.0 | \tAvarage in last 100 is -9.741573033707866action_prob: [6.8594341e-07 1.1831701e-06 1.0616291e-06 9.9999702e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1189816e-06 1.9289294e-06 1.7384378e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1175706e-06 1.9265503e-06 1.7362424e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1161667e-06 1.9241800e-06 1.7340615e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1147636e-06 1.9218123e-06 1.7318799e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1133623e-06 1.9194495e-06 1.7297044e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1119647e-06 1.9170932e-06 1.7275333e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1105636e-06 1.9147287e-06 1.7253551e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1091612e-06 1.9123638e-06 1.7231746e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1077522e-06 1.9099871e-06 1.7209854e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 89 episode: -10.0 | \tAvarage in last 100 is -9.744444444444444action_prob: [6.7710624e-07 1.1682579e-06 1.0479489e-06 9.9999714e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1049171e-06 1.9052061e-06 1.7165810e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1034818e-06 1.9027874e-06 1.7143494e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1020369e-06 1.9003519e-06 1.7121013e-06 9.9999523e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.1005855e-06 1.8979089e-06 1.7098465e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0991296e-06 1.8954561e-06 1.7075830e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0976672e-06 1.8929958e-06 1.7053095e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0962048e-06 1.8905314e-06 1.7030342e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0947442e-06 1.8880737e-06 1.7007635e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0932815e-06 1.8856121e-06 1.6984895e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 90 episode: -10.0 | \tAvarage in last 100 is -9.747252747252746action_prob: [6.6797304e-07 1.1528701e-06 1.0338036e-06 9.9999714e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0903369e-06 1.8806589e-06 1.6939084e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0888572e-06 1.8781676e-06 1.6916064e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0873703e-06 1.8756688e-06 1.6892945e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0858790e-06 1.8731590e-06 1.6869762e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0843753e-06 1.8706295e-06 1.6846353e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0828655e-06 1.8680890e-06 1.6822866e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0813546e-06 1.8655503e-06 1.6799378e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0798439e-06 1.8630079e-06 1.6775875e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0783383e-06 1.8604760e-06 1.6752469e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 91 episode: -10.0 | \tAvarage in last 100 is -9.75action_prob: [6.5856352e-07 1.1370365e-06 1.0192252e-06 9.9999714e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0753386e-06 1.8554297e-06 1.6705820e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0738526e-06 1.8529294e-06 1.6682703e-06 9.9999535e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0723647e-06 1.8504237e-06 1.6659557e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0708828e-06 1.8479284e-06 1.6636503e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0693999e-06 1.8454328e-06 1.6613451e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0679252e-06 1.8429478e-06 1.6590525e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0664526e-06 1.8404678e-06 1.6567629e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0649819e-06 1.8379910e-06 1.6544767e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0635143e-06 1.8355212e-06 1.6521951e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 92 episode: -10.0 | \tAvarage in last 100 is -9.75268817204301action_prob: [6.4928292e-07 1.1214036e-06 1.0048497e-06 9.9999726e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0605953e-06 1.8306019e-06 1.6476556e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0591357e-06 1.8281454e-06 1.6453851e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0576762e-06 1.8256853e-06 1.6431177e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0562206e-06 1.8232355e-06 1.6408534e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0547591e-06 1.8207751e-06 1.6385796e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0532945e-06 1.8183059e-06 1.6363028e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0518329e-06 1.8158452e-06 1.6340323e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0503644e-06 1.8133723e-06 1.6317493e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0488889e-06 1.8108871e-06 1.6294541e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 93 episode: -10.0 | \tAvarage in last 100 is -9.75531914893617action_prob: [6.4008782e-07 1.1059113e-06 9.9060674e-07 9.9999726e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0459172e-06 1.8058856e-06 1.6248330e-06 9.9999547e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0444252e-06 1.8033732e-06 1.6225121e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0429363e-06 1.8008640e-06 1.6201957e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0414354e-06 1.7983411e-06 1.6178627e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0399398e-06 1.7958234e-06 1.6155361e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0384404e-06 1.7932991e-06 1.6132036e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0369470e-06 1.7907886e-06 1.6108824e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0354578e-06 1.7882850e-06 1.6085672e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0339776e-06 1.7857934e-06 1.6062618e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 94 episode: -10.0 | \tAvarage in last 100 is -9.757894736842106action_prob: [6.3073355e-07 1.0901601e-06 9.7611417e-07 9.9999726e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0310197e-06 1.7808154e-06 1.6016605e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0295468e-06 1.7783393e-06 1.5993709e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0280721e-06 1.7758581e-06 1.5970770e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0265966e-06 1.7733770e-06 1.5947803e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0251183e-06 1.7708893e-06 1.5924824e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0236422e-06 1.7684050e-06 1.5901846e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0221721e-06 1.7659310e-06 1.5878994e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0207119e-06 1.7634707e-06 1.5856265e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0192538e-06 1.7610170e-06 1.5833615e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 95 episode: -10.0 | \tAvarage in last 100 is -9.760416666666666action_prob: [6.2151366e-07 1.0746375e-06 9.6182737e-07 9.9999738e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0163507e-06 1.7561265e-06 1.5788454e-06 9.9999559e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0149075e-06 1.7536950e-06 1.5766007e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0134742e-06 1.7512816e-06 1.5743740e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0120427e-06 1.7488699e-06 1.5721504e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0106269e-06 1.7464831e-06 1.5699464e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0092149e-06 1.7441013e-06 1.5677516e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0078011e-06 1.7417161e-06 1.5655538e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0063911e-06 1.7393390e-06 1.5633606e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0049755e-06 1.7369538e-06 1.5611615e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 96 episode: -10.0 | \tAvarage in last 100 is -9.762886597938145action_prob: [6.1258567e-07 1.0595833e-06 9.4800254e-07 9.9999738e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0021598e-06 1.7322046e-06 1.5567830e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [1.0007511e-06 1.7298307e-06 1.5545932e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.9934050e-07 1.7274501e-06 1.5524005e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.9793101e-07 1.7250745e-06 1.5502079e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.9652163e-07 1.7226989e-06 1.5480186e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.9511033e-07 1.7203183e-06 1.5458235e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.9368970e-07 1.7179231e-06 1.5436151e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.9226941e-07 1.7155278e-06 1.5414056e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.9083957e-07 1.7131180e-06 1.5391831e-06 9.9999571e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 97 episode: -10.0 | \tAvarage in last 100 is -9.76530612244898action_prob: [6.0371792e-07 1.0446233e-06 9.3426604e-07 9.9999738e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.8798353e-07 1.7083054e-06 1.5347421e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.8655141e-07 1.7058926e-06 1.5325130e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.8510918e-07 1.7034606e-06 1.5302699e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.8366161e-07 1.7010256e-06 1.5280211e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.8221517e-07 1.6985891e-06 1.5257727e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.8076714e-07 1.6961496e-06 1.5235190e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.7931843e-07 1.6937089e-06 1.5212643e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.7786813e-07 1.6912684e-06 1.5190100e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.7642373e-07 1.6888346e-06 1.5167633e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 98 episode: -10.0 | \tAvarage in last 100 is -9.767676767676768action_prob: [5.9467914e-07 1.0293887e-06 9.2026335e-07 9.9999750e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.7355155e-07 1.6839937e-06 1.5122973e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.7211068e-07 1.6815671e-06 1.5100564e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.7066732e-07 1.6791313e-06 1.5078101e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.6922622e-07 1.6767023e-06 1.5055699e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.6779638e-07 1.6742896e-06 1.5033474e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.6636700e-07 1.6718802e-06 1.5011240e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.6493773e-07 1.6694712e-06 1.4989024e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.6351050e-07 1.6670624e-06 1.4966827e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      "action_prob: [9.6207634e-07 1.6646444e-06 1.4944534e-06 9.9999583e-01]\n",
      "chosen_action: 3\n",
      " Total score (averaged over agents) 99 episode: -10.0 | \tAvarage in last 100 is -9.77"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[-10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " 13.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0,\n",
       " -10.0]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train(agents, env,  max_t=10, num_episodes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}