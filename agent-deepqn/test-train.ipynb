{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd04cf4a5c25d0443c718bc6df8c1cfde806d3459d5f26eb1cf93f3c76670b0736a",
   "display_name": "Python 3.9.2 64-bit ('drl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from environment.grid import GridEnv, Action\n",
    "from enum import Enum\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def mean(lst):\n",
    "    return sum(lst) / len(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = GridEnv(2)\n",
    "env.seed(0)\n",
    "print('Number of actions: ', env.action_space.n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 8x8)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c161ccac2f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/multi-taxi-drl/agent-deepqn/agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, eps)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0maction_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/drl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/multi-taxi-drl/agent-deepqn/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;34m\"\"\"Build a network that maps state -> action values.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/drl/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/drl/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/drl/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 8x8)"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "\n",
    "agent = Agent(state_size=env.state_space, action_size=env.action_space.n, seed=0)\n",
    "\n",
    "# watch an untrained agent\n",
    "state = env.reset()[0]\n",
    "for j in range(200):\n",
    "    action = agent.act(state)\n",
    "    states, rewards, done = env.step([action])\n",
    "    state, reward = states[0], rewards[0]\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-48, -37]\n",
      "[deque([], maxlen=100), deque([], maxlen=100)]\n",
      "Episode 1\tAverage Score: -42.50[-47, -58]\n",
      "[deque([-48], maxlen=100), deque([-37], maxlen=100)]\n",
      "Episode 2\tAverage Score: -47.50[-10, -21]\n",
      "[deque([-48, -47], maxlen=100), deque([-37, -58], maxlen=100)]\n",
      "Episode 3\tAverage Score: -36.83[-55, -44]\n",
      "[deque([-48, -47, -10], maxlen=100), deque([-37, -58, -21], maxlen=100)]\n",
      "Episode 4\tAverage Score: -40.00[-9, 2]\n",
      "[deque([-48, -47, -10, -55], maxlen=100), deque([-37, -58, -21, -44], maxlen=100)]\n",
      "Episode 5\tAverage Score: -32.70[-230, -241]\n",
      "[deque([-48, -47, -10, -55, -9], maxlen=100), deque([-37, -58, -21, -44, 2], maxlen=100)]\n",
      "Episode 6\tAverage Score: -66.50[-6, 5]\n",
      "[deque([-48, -47, -10, -55, -9, -230], maxlen=100), deque([-37, -58, -21, -44, 2, -241], maxlen=100)]\n",
      "Episode 7\tAverage Score: -57.07[-72, -83]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5], maxlen=100)]\n",
      "Episode 8\tAverage Score: -59.62[-337, -348]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83], maxlen=100)]\n",
      "Episode 9\tAverage Score: -91.06[-14, -25]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348], maxlen=100)]\n",
      "Episode 10\tAverage Score: -83.90[-10, 1]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25], maxlen=100)]\n",
      "Episode 11\tAverage Score: -76.68[-164, -175]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1], maxlen=100)]\n",
      "Episode 12\tAverage Score: -84.42[-12, -23]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175], maxlen=100)]\n",
      "Episode 13\tAverage Score: -79.27[-25, -36]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23], maxlen=100)]\n",
      "Episode 14\tAverage Score: -75.79[-455, -466]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36], maxlen=100)]\n",
      "Episode 15\tAverage Score: -101.43[-193, -204]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466], maxlen=100)]\n",
      "Episode 16\tAverage Score: -107.50[3, -8]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204], maxlen=100)]\n",
      "Episode 17\tAverage Score: -101.32[-101, -90]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8], maxlen=100)]\n",
      "Episode 18\tAverage Score: -101.00[-23, -12]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90], maxlen=100)]\n",
      "Episode 19\tAverage Score: -96.61[-101, -90]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12], maxlen=100)]\n",
      "Episode 20\tAverage Score: -96.55[-30, -19]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90], maxlen=100)]\n",
      "Episode 21\tAverage Score: -93.12[-104, -93]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19], maxlen=100)]\n",
      "Episode 22\tAverage Score: -93.36[-11, -22]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93], maxlen=100)]\n",
      "Episode 23\tAverage Score: -90.02[-267, -256]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22], maxlen=100)]\n",
      "Episode 24\tAverage Score: -97.17[-105, -116]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256], maxlen=100)]\n",
      "Episode 25\tAverage Score: -97.70[-158, -147]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116], maxlen=100)]\n",
      "Episode 26\tAverage Score: -99.81[7, -4]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147], maxlen=100)]\n",
      "Episode 27\tAverage Score: -96.06[-41, -30]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4], maxlen=100)]\n",
      "Episode 28\tAverage Score: -93.89[-4, 7]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30], maxlen=100)]\n",
      "Episode 29\tAverage Score: -90.60[-240, -229]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7], maxlen=100)]\n",
      "Episode 30\tAverage Score: -95.40[-54, -65]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229], maxlen=100)]\n",
      "Episode 31\tAverage Score: -94.24[-117, -128]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65], maxlen=100)]\n",
      "Episode 32\tAverage Score: -95.12[-197, -208]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128], maxlen=100)]\n",
      "Episode 33\tAverage Score: -98.38[-36, -47]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208], maxlen=100)]\n",
      "Episode 34\tAverage Score: -96.71[-45, -34]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47], maxlen=100)]\n",
      "Episode 35\tAverage Score: -95.07[-26, -15]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34], maxlen=100)]\n",
      "Episode 36\tAverage Score: -93.00[-690, -679]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15], maxlen=100)]\n",
      "Episode 37\tAverage Score: -108.99[-5, -16]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679], maxlen=100)]\n",
      "Episode 38\tAverage Score: -106.39[-234, -245]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16], maxlen=100)]\n",
      "Episode 39\tAverage Score: -109.81[-180, -169]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245], maxlen=100)]\n",
      "Episode 40\tAverage Score: -111.42[-407, -396]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169], maxlen=100)]\n",
      "Episode 41\tAverage Score: -118.50[-7, 4]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396], maxlen=100)]\n",
      "Episode 42\tAverage Score: -115.71[-158, -147]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4], maxlen=100)]\n",
      "Episode 43\tAverage Score: -116.57[-1061, -1072]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7, -158], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4, -147], maxlen=100)]\n",
      "Episode 44\tAverage Score: -138.16[-14, -25]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7, -158, -1061], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4, -147, -1072], maxlen=100)]\n",
      "Episode 45\tAverage Score: -135.52[-7, 4]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7, -158, -1061, -14], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4, -147, -1072, -25], maxlen=100)]\n",
      "Episode 46\tAverage Score: -132.61[-8, -19]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7, -158, -1061, -14, -7], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4, -147, -1072, -25, 4], maxlen=100)]\n",
      "Episode 47\tAverage Score: -130.07[9, -2]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7, -158, -1061, -14, -7, -8], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4, -147, -1072, -25, 4, -19], maxlen=100)]\n",
      "Episode 48\tAverage Score: -127.29[-46, -57]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7, -158, -1061, -14, -7, -8, 9], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4, -147, -1072, -25, 4, -19, -2], maxlen=100)]\n",
      "Episode 49\tAverage Score: -125.74[-57, -46]\n",
      "[deque([-48, -47, -10, -55, -9, -230, -6, -72, -337, -14, -10, -164, -12, -25, -455, -193, 3, -101, -23, -101, -30, -104, -11, -267, -105, -158, 7, -41, -4, -240, -54, -117, -197, -36, -45, -26, -690, -5, -234, -180, -407, -7, -158, -1061, -14, -7, -8, 9, -46], maxlen=100), deque([-37, -58, -21, -44, 2, -241, 5, -83, -348, -25, 1, -175, -23, -36, -466, -204, -8, -90, -12, -90, -19, -93, -22, -256, -116, -147, -4, -30, 7, -229, -65, -128, -208, -47, -34, -15, -679, -16, -245, -169, -396, 4, -147, -1072, -25, 4, -19, -2, -57], maxlen=100)]\n",
      "Episode 50\tAverage Score: -124.26"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f91d6238e1f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# plot the scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-f91d6238e1f9>\u001b[0m in \u001b[0;36mdqn\u001b[0;34m(agents, num_agents, n_episodes, max_t, eps_start, eps_end, eps_decay)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0meps_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-f91d6238e1f9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0meps_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/multi-taxi-drl/agent-deepqn/agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/multi-taxi-drl/agent-deepqn/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Minimize the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/drl/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/drl/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_agents = 2\n",
    "agents = [Agent(state_size=env.state_space, action_size=env.action_space.n, seed=0) for _ in np.arange(num_agents)]\n",
    "def flatten(list_of_lists):\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "\n",
    "def dqn(agents, num_agents = 2, n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = [[] for _ in np.arange(num_agents)]                       # list containing scores from each episode\n",
    "    scores_window = [deque(maxlen=100)  for _ in np.arange(num_agents)] # last 100 scores for each agent\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = env.reset()\n",
    "        eps_scores = [0 for _ in np.arange(num_agents)]\n",
    "        for t in range(max_t):\n",
    "            actions = [agent.act(flatten(states), eps) for agent in agents]\n",
    "            next_states, rewards, done = env.step(actions)\n",
    "            [agent.step(flatten(states), action, reward, flatten(next_states), done) for agent, action, reward in zip(agents, actions, rewards)]\n",
    "            states = next_states\n",
    "            eps_scores = [score + reward for score, reward in zip(eps_scores, rewards)]\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        [scores_window[index].append(eps_scores[index]) for index, _ in enumerate(agents)]       # save most recent score\n",
    "        [scores[index].append(eps_scores[index]) for index, _ in enumerate(agents)]       # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn(agents, 2)\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(3):\n",
    "    state = env.reset()\n",
    "    for j in range(200):\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  }
 ]
}